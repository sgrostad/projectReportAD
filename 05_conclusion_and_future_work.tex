\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
This project has consisted of discussing the differences in method and advantages between forward and backward AD, the applications of AD, and implementation of AD in Julia. This implementation has been benchmarked against other AD libraries. Finally, an example where AD is used in a real world example for solving partial differential equations (PDEs) that describes the flow inside an oil reservoir has been provided.

% The differences in the approach for forward and backward AD shows that backward AD is more efficient than forward AD when we have functions with many input parameters and few output parameters. The disadvantage with backward AD is that it is much more complex to implement, and if the output parameters are many, the efficiency advantage decreases. 

My implementation of forward AD in Julia (\textit{ForwardAutoDiff}) performs well in the benchmark for evaluating the value and the Jacobian of a standard vector function with three vector inputs. For vectors of length less than 1000, but more than 50, \autoref{fig:benchmarkLongVectors} and \autoref{fig:benchmarkAllADs} show that it is the fastest implementation. \autoref{fig:benchmarkAllADs} shows that the built-in AD implementation in Julia (\textit{ForwardDiff}) has very little overhead for small vectors, but scales badly as the size of the vectors increase. It is interesting to see in the benchmarking (\autoref{fig:benchmarkADInLoop}) that there is an opportunity to gain computational efficiency by using for-loops in Julia. ForwardAutoDiff is implemented similarly as the one in MATLAB, but since MATLAB has the limitations of having a lot of overhead if you use for-loops instead of vectorizing your code, it is solely based on vectorization. Hence, it would be interesting to look further into different implementations of AD in Julia to see if it is possible to make a more efficient implementation by using for-loops in the implementation. This investigation would also consist of testing the implementations further for different types of functions that gives different structures to the Jacobians. 

For PDEs it has been demonstrated that by introducing discrete divergence and gradient operators we can solve the PDEs elegantly by using a finite volume method. Using these operators we can write the discrete equations on a very similar form as the continuous. By setting up the equations on residual form, we can solve them by using AD and the Newton-Raphson method to find the roots of the function. For the real world example in \autoref{ch:FlowSolver}, the AD implementation in MATLAB solves the problem quicker than the implementation in Julia. It would be interesting to try different implementations of AD in Julia for this and other, similar real world problems. As the benchmark in \autoref{fig:benchmarkAllADs} shows, the standard implementation of forward AD in Julia, \textit{ForwardDiff}, is performing well for small problems. Hence an interesting approach would be to use this library as a building brick of a new implementation of AD. Since Julia has proven itself to having little overhead when using for-loops, this may lead to a better implementation of AD in Julia.

\begin{itemize}
    \item Oppsummere kort hva som har blitt gjort
    \item Konkludere på hastighetsforskjellene som har blitt sett på de forskjellige AD-metodene
    \item Skrive om videre implementering for testing av AD-tool
    \item teste av andre implementasjoner av AD i Julia for å få raskere enn matlab. Spesielt mtp at for-løkker er kjapt
\end{itemize}