\chapter{Conclusion and Future Work}
\label{ch:Conclusion}
This project has consisted of discussing the differences in method and advantages between forward and backward AD, the applications of AD, and implementation of AD in Julia. The implementation has been benchmarked against other AD libraries. Finally, an example where AD is used in a real world example for solving partial differential equations (PDEs) that describes the flow inside an oil reservoir has been provided.

My implementation of forward AD in Julia (\textit{ForwardAutoDiff}) performs well in the benchmark for evaluating the value and the Jacobian of a standard vector function with three vector inputs. For vectors of length less than 1000, but more than 50, \autoref{fig:benchmarkLongVectors} and \autoref{fig:benchmarkAllADs} show that it is the fastest implementation tested. It is interesting to see in the benchmarking (\autoref{fig:benchmarkADInLoop}) that there is an opportunity to gain computational efficiency by using for-loops in Julia. \textit{ForwardAutoDiff} is implemented similarly as the one in MATLAB, but since MATLAB has the limitations of having a lot of overhead if you use for-loops, it is solely based on vectorization. As this is not the case for Julia, it would be interesting to look further into different implementations of AD in Julia to see if it is possible to make a more efficient implementation by a more extensive use of for-loops. In addition to this, \autoref{fig:benchmarkAllADs} show that the built-in AD implementation in Julia (\textit{ForwardDiff}) has very little overhead for small vectors, but scales badly as the size of the vectors increase. Even tough it scales badly for long vectors, considering the low overhead for small problems, an interesting approach would be to use this library as a building brick of a new implementation of AD. Since Julia has proven itself to have little overhead when using for-loops, this may lead to a better implementation of AD in Julia.

For PDEs it has been demonstrated that by introducing discrete divergence and gradient operators we can solve the PDEs describing the flow of oil in a reservoir elegantly by using a finite volume method and AD. Using these operators we can write the discrete equations on a very similar form as the continuous. By setting up the equations as a function on residual form, we can solve it by using AD and the Newton-Raphson method to find the roots of the function. For the real world example in \autoref{ch:FlowSolver}, the AD implementation in MATLAB solves the problem quicker than the implementation in Julia for all discretizations tested. The discretization with fewest cells, gives a system of 1002 equations. So even tough \textit{ForwardAutoDiff} was the fastest at evaluating a vector function (\autoref{fig:benchmarkAllADs}) for this length, it is still slower at solving the example. This show that when the function evaluated becomes more complex, the AD tool in MATLAB handles it better than \textit{ForwardAutoDiff}. Hence it would be interesting to see if with new implementations, either with less vectorization or with using \textit{ForwardDiff} as a building brick, the computational efficency of the flow solver example or other real world examples can be improved by using Julia.

% This investigation would also consist of testing the implementations further for different types of functions that gives different structures to the Jacobians