\chapter{Theory}
\label{ch:theory}
\section{Automatic differentiation}
\label{sec:AD}
Automatic differentiation (AD) is a method that makes the computer derive the derivatives with very little effort from the user. If you have not heard of AD before, the first thing that you might think of is algebraic or symbolic differentiation. In this type of differentiation the computer learns the basic rules from calculus like e.g.
\begin{align*}
    &\frac{d}{dx}x^n     = n\cdot x^{n-1} \\
    &\frac{d}{dx}cos(x)  = -sin(x) \\
    &\frac{d}{dx}\exp{x} = \exp{x} \\
\end{align*}
etc. and the chain- and product rule
\begin{align*}
    &\frac{d}{dx}f(x)\cdot g(x) = f'(x)\cdot g(x) + f(x)\cdot g'(x) \\
    &\frac{d}{dx}f(g(x)) = g'(x)\cdot f'(g(x)).
\end{align*}
The computer will then use these rules on symbolic variables to obtain the derivative of any function given. This will give perfectly accurate derivatives, but the disadvantage with this approach is that it is computational demanding and as f(x) becomes more complex, the calculations will become slow.

If AD is not symbolic differentiation you might think that it is finite differences, where you use the definition of the derivative
\begin{equation*}
    \frac{df}{dx} = \frac{f(x+h) - f(x)}{h}
\end{equation*}
with a small h to obtain the numerical approximation of the derivative of f. This approach is not optimal because, first of all, if you choose an h too small, you will get problems with rounding errors on your computer. This is because when h is small, you will subtract two very similar numbers, f(x+h) and f(x) and then divide by a small number h. This means that any small rounding errors in the subtraction will be hugely magnified by the division. Secondly, if you choose h too large your approximation of the derivative will not be accurate. 

AD can be split into two different methods - forward AD and backward AD. Both methods are similar to symbolic differentiation in the way that we implement the differentiation rules, but they differ by instead of differentiating symbols and then inserting values for the symbols, we keep track of the function values and the corresponding values of the derivatives as we go. Both methods does this by separating each expression into a finite set of elementary operations. 

\subsection{Forward Automatic Differentiation}
In forward AD, the function value is stored in a tuple $[\cdot, \cdot]$. In this way, we can continuously update both the function value and the derivative value for every operation we perform on the function. 

As an example, consider the function $f = f(x)$ with its derivative $f_x$ where $x$ is a scalar variable. Then the AD-variable $x$ is the pair $[x, 1]$ and for f we have [$f$, $f_x$]. In the pair $[x,1]$, $x$ is the numerical value of $x$ and $1 = \frac{dx}{dx}$. Similar for $f(x)$ where $f$ is the numerical value of $f(x)$ and $f_x$ the numerical value of $f'(x)$.  We then define the arithmetic operators for such pairs such that for functions $f$ and $g$,
\begin{equation}
    \begin{aligned}
    &\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big] \pm \big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big] = \big[f \pm g \hspace{0.5em} , \hspace{0.5em}  f_x \pm g_x\big] \hspace{0.5em}, \\\\
    &\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\cdot \big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big] = \big[f\cdot g \hspace{0.5em} , \hspace{0.5em}  f_x\cdot g + f\cdot g_x\big],\\\\
    &\frac{\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]}{\big[g \hspace{0.5em} , \hspace{0.5em}  g_x\big]} = \Bigg[\frac{f}{g} \hspace{0.5em} , \hspace{0.5em}  \frac{f_x\cdot g - f\cdot g_x}{g^2}\Bigg].
\end{aligned}
\label{eq:arithmeticOperators}
\end{equation}

It is also necessary to define the chain rule such that for a function h(x)
\begin{equation*}
h\big(f(x)\big) = h\bigg(\big[f \hspace{0.5em} , \hspace{0.5em} f_x\big]\bigg) = \big[h(f) \hspace{0.5em} , \hspace{0.5em}  f_x\cdot h'(f)\big]. 
\end{equation*}
The only thing that remains to define are the rules concerning elementary functions like
\begin{equation}
    \begin{aligned}
    &\exp\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \big[\exp(f) \hspace{0.5em} , \hspace{0.5em}  \exp(f)\cdot f_x\big],\\
    &\log\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \Big[\log(f) \hspace{0.5em} , \hspace{0.5em}  \frac{f_x}{f}\Big],\\
    &\sin\bigg(\big[f \hspace{0.5em} , \hspace{0.5em}  f_x\big]\bigg) =  \big[\sin(f) \hspace{0.5em} , \hspace{0.5em}  \sin(f)\cdot f_x\big]\text{,  etc.}\\
\end{aligned}
\label{eq:elementaryFunctions}
\end{equation}

When these arithmetic operators and the elementary function are implemented you are able to calculate any scalar function derivative without actually doing any form of differentiation yourselves. Let us look at an step by step example where 
\begin{equation}
    \label{eq:forwardADExample}
    f(x) = x\cdot\exp(2x) \hspace{3em} \text{for}\hspace{1em} x = 2.
\end{equation}
Then the declaration of the AD-variable x gives $x = [2 \hspace{0.5em} , \hspace{0.5em}  1]$. All scalars can be looked at as AD variables with derivative equal to 0 such that
\begin{align*}
    2x &= [2 \hspace{0.5em} , \hspace{0.5em}  0]\cdot [2 \hspace{0.5em} , \hspace{0.5em} 1] \\
    &=[2\cdot2 \hspace{0.5em} , \hspace{0.5em}  0\cdot1 + 2\cdot 1]\\
    &=[4 \hspace{0.5em} , \hspace{0.5em} 2].
\end{align*}
After this computation we get from the exponential
\begin{align*}
    \exp(2x) &= \exp\big([4 \hspace{0.5em} , \hspace{0.5em} 2]\big)\\
    &= [\exp(4) \hspace{0.5em} , \hspace{0.5em} \exp(4)\cdot 2],
\end{align*}
and lastly from the product rule we get the correct tuple for $f(x)$
\begin{align*}
    x\cdot \exp(2x) &= [2 \hspace{0.5em} , \hspace{0.5em}  1]\cdot [\exp(4)\hspace{0.5em}, \hspace{0.5em} 2\cdot\exp(4)]\\
    &=[2\cdot\exp(4) \hspace{0.5em} , \hspace{0.5em}  1\cdot \exp(4) + 2\cdot 2\dcot\exp(4)]\\
    [f\hspace{0.5em},\hspace{0.5em} f_x]&=[2\cdot\exp(4) \hspace{0.5em} , \hspace{0.5em} 5\cdot\exp(4)].
\end{align*}
This result is equal
\begin{equation*}
    \big(f(x) \hspace{0.5em},\hspace{0.5em} f_x(x)\big) = \big(x\cdot\exp(2x) \hspace{0.5em},\hspace{0.5em} (1+2x)\exp(2x)\big)
\end{equation*}
for $x = 2$.


\subsection{Dual Numbers}
One approach to implementing forward AD is by dual numbers. Similar to complex numbers dual numbers are defined as 
\begin{equation}
    a + b\epsilon.
    \label{eq:DualNumbers}
\end{equation}
Here $a$ and $b$ are scalars and corresponds to the function value and the derivative value. $\epsilon$ is like we have for complex numbers $i^2 = -1$, but the corresponding relation for dual numbers are $\epsilon^2 = 0$. The convenient part of implementing forward AD with dual numbers is that you get the differentiation rules for arithmetic operations for free. Consider the dual numbers $x$ and $y$ on the form of definition \eqref{eq:DualNumbers}. Then we get for addition
\begin{align*}
x+y &= (a+b\epsilon)+(c+d\epsilon)\\
    &= a+c+(b+d)\epsilon,
\end{align*}
for multiplication
\begin{align*}
x\cdot y &= (a+b\epsilon)\cdot(c+d\epsilon)\\
    &= ac + (ad + bc)\epsilon + bd\epsilon^2 \\
    &= ac + (ad + bc)\epsilon,
\end{align*}
and for division
\begin{align*}
\frac{x}{y} &= \frac{a+b\epsilon}{c+d\epsilon}\\
    &=\frac{a+b\epsilon}{c+d\epsilon} \cdot \frac{c-d\epsilon}{c-d\epsilon}\\
    &=\frac{ac-(ad-bc)\epsilon-bd\epsilon^2}{c^2-d\epsilon^2}\\
    &=\frac{a}{c} + \frac{bc-ad}{c^2}\epsilon.
\end{align*}
This is very convenient, but how does dual numbers handle elementary functions like $\sin$, $\exp$, $\log$ etc? If we look at the Taylor expansion of a function $f(x)$ where x is a dual number, we get
\begin{align*}
    f(x) = f(a+b\epsilon) &= f(a) + \frac{f'(a)}{1!}(b\epsilon) + \frac{f''(a)}{2!}(b\epsilon)^2+...\\
        &=f(a) + f'(a)b\epsilon.
\end{align*}
This means that to make dual numbers handle elementary functions, the first order Taylor expansion needs to be implemented. This equals the implementation of elementary differentiation rules described in equations \eqref{eq:elementaryFunctions}. 

The weakness of implementing AD with dual numbers is clear for functions with multiple variables. Let the function $f$ be defined as $f(x,y,z) = x\cdot y + z$. Let us say we want to know the function value for $(x,y,z) = (2,3,4)$ together with all the derivatives of $f$. First we evaluate $f$ with $x$ as the only varying parameter and the rest as constants:
\begin{align*}
    f(x,y,z) &= (2+1\epsilon)\cdot(3+0\epsilon) + (1+0\epsilon)\\
        &=7+3\epsilon.
\end{align*}
$7$ is now the function value of f, while 3 is the derivative value of f with respect to x, $f_x$. To obtain $f_y$ and $f_z$ we need two more function evaluations with respectively $y$ and $z$ as the varying parameters. This example illustrates the weakness of forward AD implemented with dual numbers - when the function evaluated have $n$ input variables, we need $n$ function evaluations to determine the gradient of the function.

\subsection{Backward Automatic Differentiation}
The main disadvantage with forward AD is when there are many input variables and you want the derivative with respect to all variables. This is where Backward AD is a more efficient way of obtaining the derivatives. To explain backward AD it is easier to first consider the approach for forward AD, where the method also can be be explained as an extensive use of the chain rule
\begin{equation}
    \label{eq:chainRule}
    \frac{\partial f}{\partial t} = \sum_i\left(\frac{\partial f}{\partial u_i}\cdot\frac{\partial u_i}{\partial t}\right).
\end{equation}
Take $f(x) = x\cdot\exp(2x)$ like in the forward AD example \eqref{eq:forwardADExample}. We then split up the function into a sequence of elementary functions
\begin{equation}
    \label{eq:BackwardADSeperationSimple}
    x, \hspace{2em} g_1 = 2x, \hspace{2em} g_2 = \exp(g_1), \hspace{2em} g_3 = x\cdot g_2,
\end{equation}
where clearly $f(x) = g_3$. If we want the derivative of $f$ with respect to $x$ we can obtain expressions for all $g$'s by using the chain rule \eqref{eq:chainRule}
\begin{align*}
     \frac{\partial x}{\partial x} &= 1, \\
     \frac{\partial g_1}{\partial x} &= 2, \\
     \frac{\partial g_2}{\partial x} &= \frac{\partial}{\partial g_1}\exp(g_1)\cdot\frac{\partial g_1}{\partial x} = 2\exp(2x).
\end{align*}
Lastly by calculating the derivative of $g_3$ with respect to $x$ in the same matter yields the expression for the derivative of f
\begin{align*}
    \frac{\partial f}{\partial x} &= \frac{\partial g_3}{\partial x}\\
    &=\frac{\partial x}{\partial x}\cdot g_2 + x\cdot\frac{\partial g_2}{\partial x}\\
    &= \exp(2x) + x\cdot 2\exp(2x) \\
    &= (1+2x)\exp(2x).
\end{align*}
This shows how forward AD actually uses the chain rule on a sequence of elementary functions with respect to the independent variables, in this case $x$. Backward AD also uses the chain rule, but in the opposite direction. It uses it with respect to dependent variables. The chain rule then has the form
\begin{equation}
    \label{eq:chainRuleReverse}
    \frac{\partial s}{\partial u} = \sum_i\left(\frac{\partial f_i}{\partial u}\cdot\frac{\partial s}{\partial f_i}\right),
\end{equation}
for some s to be chosen. In the same example with $f(x) = x\cdot\exp(2x)$ and with the same sequence of elementary functions like in \eqref{eq:BackwardADSeperationSimple} gives the expressions from the chain rule \eqref{eq:chainRuleReverse}
\begin{align*}
    \frac{\partial s}{\partial g_3} &= \text{Unknown}\\
    \frac{\partial s}{\partial g_2} &= \frac{\partial g_3}{\partial g_2} \cdot \frac{\partial s}{\partial g_3} &&\Longleftrightarrow \quad x\cdot \frac{\partial s}{\partial g_3} \\
    \frac{\partial s}{\partial g_1} &= \frac{\partial g_2}{\partial g_1}\cdot \frac{\partial s}{\partial g_2} &&\Longleftrightarrow \quad g_2 \cdot \frac{\partial s}{\partial g_2} \\
    \frac{\partial s}{\partial x} &= \frac{\partial g_3}{\partial x}\cdot \frac{\partial s}{\partial g_3} + \frac{\partial g_1}{\partial x}\cdot \frac{\partial s}{\partial g_1} && \Longleftrightarrow \quad g_2\cdot \frac{\partial s}{\partial g_3} + 2\cdot \frac{\partial s}{\partial g_1}.\\
\end{align*}
By substituting $s$ with $g_3$ gives
\begin{align*}
    \frac{\partial g_3}{\partial g_3} &= 1\\
    \frac{\partial g_3}{\partial g_2} &= x\\
    \frac{\partial g_3}{\partial g_1} &= \exp(2x)\cdot x\\
    \frac{\partial g_3}{\partial x} &= \exp(2x)\cdot 1 + 2\cdot\exp(2x)\cdot x = (1 + 2x)\exp(2x),\\
\end{align*}
hence we obtain the correct derivative $f_x$. By now you might wonder why make this much effort to obtain the derivative of f compared to just using forward AD. The answer to this comes by looking at a more complex function with multiple input parameters. Let $f(x,y,z) = z(\sin(x^2)+yx)$ and 
\begin{equation*}
    g_1 = x^2, \quad g_2 = x\cdot y, \quad g_3 = \sin(g_1), \quad g_4 = g_2 + g_3, \quad g_5 = z\cdot g_4.
\end{equation*}
Now the derivatives from the chain rule in equation \eqref{eq:chainRuleReverse} becomes \\
\begin{multicols}{3}
    \noindent
    \begin{align*}
        \frac{\partial s}{\partial g_5} &= \text{Unknown}\\
        \frac{\partial s}{\partial g_4} &= z\cdot\frac{\partial s}{\partial g_5}\\
        \frac{\partial s}{\partial g_3} &= \frac{\partial s}{\partial g_4}
    \end{align*}
    \begin{align*}
        \frac{\partial s}{\partial g_2} &= \frac{\partial s}{\partial g_4} \\
        \frac{\partial s}{\partial g_1} &= \cos(g_1)\frac{\partial s}{\partial g_3}\\
        \frac{\partial s}{\partial x}   &= 2x\cdot\frac{\partial s}{\partial g_1} + y\cdot\frac{\partial s}{\partial g_2}
    \end{align*}
    \begin{align*}
        \frac{\partial s}{\partial y}   &= x\cdot\frac{\partial s}{\partial g_2}\\
        \frac{\partial s}{\partial z}   &= g_4\cdot\frac{\partial s}{\partial g_5}
    \end{align*}
\end{multicols}

substituting $s$ with $g_5$ yields
\begin{multicols}{3}
    \noindent
    \begin{align*}
        \frac{\partial g_5}{\partial g_5} &= 1\\
        \frac{\partial g_5}{\partial g_4} &= z\\
        \frac{\partial g_5}{\partial g_3} &= z
    \end{align*}
    \begin{align*}
        \frac{\partial g_5}{\partial g_2} &= z\\
        \frac{\partial g_5}{\partial g_1} &= \cos(x^2)\cdot z\\
        \frac{\partial g_5}{\partial x}   &= 2x\cdot\cos(x^2)\cdot z + yz
    \end{align*}
    \begin{align*}
        \frac{\partial g_5}{\partial y}   &= xz\\
        \frac{\partial g_5}{\partial z}   &= \sin(x^2)+xy
    \end{align*}
\end{multicols}
The calculation of the derivatives together with a dependency graph can be seen in figure \ref{fig:depency_graph_backward_AD}. This shows that we get all the derivatives of $f(x) = g_5$ with a single function evaluation!
\begin{figure}[htbp]
	\centering
	    \includegraphics[width=0.9\textwidth]{figures/dependency_graph_backward_AD.pdf}
	    \caption{NOT A FINISHED FIGURE. Just a quick sketch of the idea I have of visualising the process of backward AD as i felt it got a bit messy with all the expressions. Gladly receiving comments on the thought/suggestions to make it more clear}
	\label{fig:depency_graph_backward_AD}
\end{figure}
Comparing this to the method of Dual Numbers were we would have to evaluate $f$ three times, one for each derivative, this is a big improvement. This illustrates the strength of backward AD - no matter how many input parameters a function have, you only need one function evaluation to get all the derivatives of a function. The disadvantage of backward AD is that, in differ from what we did in the example above when we did it by hand, we still only want to carry along the function value and the derivative value as we did in forward AD. This means that we have to implement the dependency tree shown in figure \ref{fig:depency_graph_backward_AD}. This makes the implementation of backward AD much harder than for forward AD and a bad implementation of this tree will reduce the advantage of backward AD. Also if the function is a vector function and not a scalar function, backward AD needs to run $m$ times if $f: \Re^n \rightarrow \Re^m$, hence if $n\approx m$, forward AD and backward AD will have approximately the same complexity. This is some of the reason why we here will have focus on implementing forward AD.

\subsection{Forward Automatic Differentiation with multiple parameters}
When we are dealing with functions with many input parameters and we wish to implement a forward AD, there are more efficient ways of doing this than implementing with Dual Numbers. Knut-Andreas Lie describes in \emph{\citet{lieMrstUrl}} a method where we do not need $n$ function evaluations for $n$ input parameters. Say we have a scalar function $f: \Re^n \rightarrow \Re$ and we want to obtain the gradient of $f$. Then the main idea is when we define our AD-variables, instead of having each AD-variable storing only the derivative with respect to itself, they store their gradient to the corresponding space they are in. This means we have to define what we call our primary variables which is all the variables in the relevant space. Say we have three variables $x$, $y$ and $z$ and for any function $f(x,y,z)$ we are interested in finding the gradient of $f$, $\nabla f=(f_x,\hspace{0.5em} f_y,\hspace{0.5em} f_z)^\top$. To achieve this we define the corresponding AD-variables
\begin{equation*}
    [x\hspace{0.5em},\hspace{0.5em}(1,0,0)^\top]\hspace{2em},\hspace{2em}
    [y\hspace{0.5em},\hspace{0.5em}(0,1,0)^\top]\hspace{2em},\hspace{2em}
    [z\hspace{0.5em},\hspace{0.5em}(0,0,1)^\top].
\end{equation*}
Each primary AD-variable now not only store their derivative with respect to themselves, but the gradient. The operators defined in Equations \ref{eq:arithmeticOperators} and the elementary functions in Equations \ref{eq:elementaryFunctions} are still valid, but instead of scalar products there are now vector products. As an example let $f(x,y,z) = xyz$ and $x = 1$, $y = 2$ and $z = 3$, then
\begin{align*}
    xyz &= [1\hspace{0.5em},\hspace{0.5em}(1,0,0)^\top]\cdot [2\hspace{0.5em},\hspace{0.5em}(0,1,0)^\top]\cdot
    [3\hspace{0.5em},\hspace{0.5em}(0,0,1)^\top] \\
    &=[1\cdot 2\cdot 3\hspace{0.5em},\hspace{0.5em}2\cdot3\cdot(1,0,0)^\top + 1\cdot3\cdot(0,1,0)^\top + 1\cdot2\cdot(0,0,1)^\top] \\
    [f\hspace{0.5em},\hspace{0.5em}\nabla f] &= [6\hspace{0.5em},\hspace{0.5em}(6,3,2)^\top].
\end{align*}
This result is equal to the tuple
\begin{equation*}
    \big(f(x,y,z) \hspace{0.5em},\hspace{0.5em} \nabla f(x,y,z)\big) = \big(xyz \hspace{0.5em},\hspace{0.5em} (yz,xz,xy)^\top\big)
\end{equation*}
for the corresponding $x$, $y$ and $z$ values. In numerical applications, as we are dealing with discretizations, the functions we evaluate are usually  vector functions and not scalar functions, hence $f: \Re^n \rightarrow \Re^m$. Then it is the Jacobian of $f$ we are interested in
\begin{align*}
    \mat{J}  =
    \begin{bmatrix}
        \frac{\partial f_1}{\partial x_1} & \dotsb & \frac{\partial f_1}{x_n}\\
        \vdots & \ddots & \vdots \\
        \frac{\partial f_m}{\partial x_1} & \dotsb & \frac{\partial f_m}{x_n}
    \end{bmatrix}.
\end{align*}
The forward AD method described above will be similar for a vector function and a scalar function as above, but there will be two differences in particular. The first one is that the primal variables needs to be initialised with their Jacobians and not just a gradient vector. The Jacobian for a primary variable of dimension $n$ will be the $n \times n$ identity matrix. The second change is that when evaluating new functions depending on the primary variables, the Jacobians corresponding to the functions will be calculated with matrix operations instead of vector operations as seen above. 



















