\chapter{Introduction}
Automatic differentiation(AD) is a method that automatically calculate the derivatives of a function with no human calculations. It is however not the method of finite differences nor symbolic differentiation. The method consist of separating an expression into a finite set of elementary operations +,-,* and / and elementary functions like the exponential and the logarithm, it then performs standard differentiation rules to these operations and functions. But it does not apply differentiation rules to the symbols like we do when we calculate it by hand. It carries along both the function value and derivative values for all the elementary steps of the evaluation. In this way it can calculate the next function value and derivative value based on the current values and the rules for the next elementary operation or function. This gives derivative values as accurate as hand derived derivatives, but with the loss of possible human error and with low computational cost. AD can be split into two different methods -- backward AD and forward AD. They both obtain the derivatives, but with different approaches that have different capabilities. The exact difference and capabilities between the two will be discussed closer in \autoref{sec:AD}. 

According to \emph{\citep{SurveyAD}} the first ideas of the concept Automatic Differentiation dates back to the 1950's \emph{\citep{nolan1953analytical}, \citep{beda1959programs}}. More specifically forward AD was discovered by Wengert in 1964 \emph{\citep{wengert1964simple}}. It is more difficult to date exactly when Backward AD was discovered, but the first article containing the essence of backward AD dates back to the 1960's \emph{\citep{boltyanskii1960theory}}. After the initial discovery of AD it died out for a couple of years before it was rediscovered along with the birth of modern computers and computer languages. The first running computer program that used backward AD and automatically computed the derivatives came in 1980 by Speelpenning \emph{\citep{speelpenning1980compiling}} and further research on the topic was done by among others Griewank during the 1980's \emph{\citep{griewank1989automatic}}. Today AD is widely used in many applications. One of them is machine learning that specifically uses the backward AD to minimize functions. This projects main focus has been to use the forward AD method to solve partial differential equations. 

The report begins with describing the theory behind AD and the difference between backward and forward AD in \autoref{ch:theory}. The chapter continues describing different applications of AD and how we can use forward AD to elegantly solve partial differential equations using a finite volume method. In \autoref{ch:Implementation} I start by elaborating why we want to implement AD in Julia. Some implementation specific details are then discussed before the implementation is tested against other AD implementations in Julia and MATLAB. The implemented AD library will then be used in \autoref{ch:FlowSolver} to solve a real world example that consist of solving partial differential equations that describes the flow inside an oil reservoir using a finite volume method. Lastly \autoref{ch:Conclusion} summarizes the results and gives some thoughts on what topics are interesting to look further into.



\listoftodos